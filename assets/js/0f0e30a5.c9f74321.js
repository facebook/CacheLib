"use strict";(self.webpackChunkcachelib=self.webpackChunkcachelib||[]).push([[4031],{15680:(e,a,t)=>{t.r(a),t.d(a,{MDXContext:()=>m,MDXProvider:()=>d,mdx:()=>f,useMDXComponents:()=>h,withMDXComponents:()=>c});var n=t(96540);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function i(){return i=Object.assign||function(e){for(var a=1;a<arguments.length;a++){var t=arguments[a];for(var n in t)Object.prototype.hasOwnProperty.call(t,n)&&(e[n]=t[n])}return e},i.apply(this,arguments)}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function l(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){o(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,o=function(e,a){if(null==e)return{};var t,n,o={},i=Object.keys(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||(o[t]=e[t]);return o}(e,a);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)t=i[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var m=n.createContext({}),c=function(e){return function(a){var t=h(a.components);return n.createElement(e,i({},a,{components:t}))}},h=function(e){var a=n.useContext(m),t=a;return e&&(t="function"==typeof e?e(a):l(l({},a),e)),t},d=function(e){var a=h(e.components);return n.createElement(m.Provider,{value:a},e.children)},p={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},u=n.forwardRef((function(e,a){var t=e.components,o=e.mdxType,i=e.originalType,r=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=h(t),d=o,u=c["".concat(r,".").concat(d)]||c[d]||p[d]||i;return t?n.createElement(u,l(l({ref:a},m),{},{components:t})):n.createElement(u,l({ref:a},m))}));function f(e,a){var t=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var i=t.length,r=new Array(i);r[0]=u;var l={};for(var s in a)hasOwnProperty.call(a,s)&&(l[s]=a[s]);l.originalType=e,l.mdxType="string"==typeof e?e:o,r[1]=l;for(var m=2;m<i;m++)r[m]=t[m];return n.createElement.apply(null,r)}return n.createElement.apply(null,t)}u.displayName="MDXCreateElement"},99230:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>c,contentTitle:()=>s,default:()=>p,frontMatter:()=>l,metadata:()=>m,toc:()=>h});var n=t(9668),o=t(21367),i=(t(96540),t(15680)),r=["components"],l={id:"ram_cache_design",title:"RAM Cache Design"},s=void 0,m={unversionedId:"Cache_Library_Architecture_Guide/ram_cache_design",id:"Cache_Library_Architecture_Guide/ram_cache_design",title:"RAM Cache Design",description:"This is a high level document that explains the design of different components within RAM cache and how they fit together and work. This is a good place to learn about the language for terms in the code as well.",source:"@site/docs/Cache_Library_Architecture_Guide/RAM_cache_design.md",sourceDirName:"Cache_Library_Architecture_Guide",slug:"/Cache_Library_Architecture_Guide/ram_cache_design",permalink:"/docs/Cache_Library_Architecture_Guide/ram_cache_design",draft:!1,editUrl:"https://github.com/facebook/CacheLib/edit/main/website/docs/Cache_Library_Architecture_Guide/RAM_cache_design.md",tags:[],version:"current",frontMatter:{id:"ram_cache_design",title:"RAM Cache Design"},sidebar:"archguideSideBar",previous:{title:"Cachelib Common Components",permalink:"/docs/Cache_Library_Architecture_Guide/common_components"},next:{title:"RAM cache indexing and eviction",permalink:"/docs/Cache_Library_Architecture_Guide/ram_cache_indexing_and_eviction"}},c={},h=[{value:"Requirements for a cache",id:"requirements-for-a-cache",level:2},{value:"Non-Goals",id:"non-goals",level:2},{value:"Major Components",id:"major-components",level:2},{value:"Shared Memory Management",id:"shared-memory-management",level:2},{value:"Memory Allocator",id:"memory-allocator",level:2},{value:"CacheAllocator",id:"cacheallocator",level:2},{value:"Slab Release",id:"slab-release",level:3},{value:"Pool rebalancer",id:"pool-rebalancer",level:3},{value:"Pool resizer",id:"pool-resizer",level:3}],d={toc:h};function p(e){var a=e.components,t=(0,o.A)(e,r);return(0,i.mdx)("wrapper",(0,n.A)({},d,t,{components:a,mdxType:"MDXLayout"}),(0,i.mdx)("p",null,"This is a high level document that explains the design of different components within RAM cache and how they fit together and work. This is a good place to learn about the language for terms in the code as well."),(0,i.mdx)("h2",{id:"requirements-for-a-cache"},"Requirements for a cache"),(0,i.mdx)("p",null,"Some of the critical goals for a cache are the following and most of the design is centered around this."),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"Must be restorable when we restart or deploy new binaries"),(0,i.mdx)("li",{parentName:"ul"},"Support high rate of evictions in the order of hundred thousand per second and be able to linearly scale"),(0,i.mdx)("li",{parentName:"ul"},"Very fast and efficient access based on a binary key"),(0,i.mdx)("li",{parentName:"ul"},"Support for isolating workloads with different cache patterns"),(0,i.mdx)("li",{parentName:"ul"},"Ability to detect change in workload and adapt, providing a good hit ratio over time.")),(0,i.mdx)("h2",{id:"non-goals"},"Non-Goals"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"Native ",(0,i.mdx)("inlineCode",{parentName:"li"},"std::")," container or data type support. We expect clients of cache library to have a malloc like interface that gives back a piece of memory and do what they need with it. The users are free to implement complex data structures on top of this and cache library will provide necessary support as we see fit."),(0,i.mdx)("li",{parentName:"ul"},"Portability across wide variety of hardware architectures. Some of the decisions around alignment are specific to the processors that we run on in production. We assume the penalty for misaligned reads are non-existent in the hardware we use. There is a small penalty for unaligned atomics, which we are ready to sacrifice for better memory utilization."),(0,i.mdx)("li",{parentName:"ul"},"Optimizing for RSS. Typically memory allocators try to avoid defragmentation and keeping the working set size of the pages minimal. However, a memory allocator for a cache is designed to be such that there is not a lot of free memory from the MemoryAllocator perspective. Therefore, it is not effective to return or advise-away free pages for the optimization. Exception is when there are high memory pressure in the system in which case we are trying to release pages to prevent the OOM.")),(0,i.mdx)("h2",{id:"major-components"},"Major Components"),(0,i.mdx)("p",null,"The components under cachelib(RAM part) can be broadly divided into these buckets."),(0,i.mdx)("ol",null,(0,i.mdx)("li",{parentName:"ol"},(0,i.mdx)("strong",{parentName:"li"},"Shared Memory Management"),": These provide ability to allocate memory in large chunks and be able to detach and re-attach to them across processes. We use these for most of our cache and storing metadata information as well. The code for this is under ",(0,i.mdx)("inlineCode",{parentName:"li"},"cachelib/shm")," directory and deals with different abstractions and implementations for shared memory."),(0,i.mdx)("li",{parentName:"ol"},(0,i.mdx)("strong",{parentName:"li"},"MemoryAllocator"),": This component takes in a large chunk of memory and implements a malloc like interface on top of that. Unlike traditional memory allocators, this one is optimized for the goals listed above for building a multi-threaded cache. The memory allocator supports pools, custom allocation sizes to help with defragmentation, and does not care much about alignment for allocations. The memory allocator also supports persisting the state across binary restarts, by integrating with the shared memory components. The last and important feature in the memory allocator is support for dynamically resizing the distribution of memory across pools and across different allocation sizes within a pool. Such re-distributions are performed by the mechanism called ",(0,i.mdx)("a",{parentName:"li",href:"#slab-release"},"slab release"),"."),(0,i.mdx)("li",{parentName:"ol"},(0,i.mdx)("strong",{parentName:"li"},"CacheAllocator"),": This is the component that provides the final API for the cache. The CacheAllocator encapsulates the memory allocator and provides apis to allocate some memory with a binary key, fetch the memory corresponding to a key. It also supports pools, resizing and rebalancing the pools. The cache is thread safe for its apis, but the user has to explicitly manage the synchronization for any mutations to the memory allocated through the cache if required.")),(0,i.mdx)("p",null,(0,i.mdx)("strong",{parentName:"p"},"GENERAL CACHE LINGO")),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},"Warm Roll: Restarting the process without losing the cache."),(0,i.mdx)("li",{parentName:"ul"},"Cold Roll: Dropping the cache on restart of the process."),(0,i.mdx)("li",{parentName:"ul"},"Arenas: Logical separation of the cache. This maps to a pool inside the cache."),(0,i.mdx)("li",{parentName:"ul"},"Item: refers to an allocation in the cache.")),(0,i.mdx)("h2",{id:"shared-memory-management"},"Shared Memory Management"),(0,i.mdx)("p",null,"The following are the main players in this area, bottom up:"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"PosixShmSegment/SysVShmSegment"),": Implementations for allocating shared memory through the POSIX api and old SYS-V api. They provide apis to create segments based on a key and be able to attach them to the process address space across binaries."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"ShmSegment"),": Provides a common interface across the two shm segment type (POSIX/SYS-V) that can be used by other components above the stack. ",(0,i.mdx)("inlineCode",{parentName:"li"},"usePosix")," parameter in the constructor determines whether POSIX or SYS-V will be used. The code can be found in ",(0,i.mdx)("inlineCode",{parentName:"li"},"cachelib/shm/Shm.h"),"."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"ShmManager"),": Provides api for managing a bunch of segments. The api supports attaching to them across restart using the same keys, managing the lifetime of the segments. The important aspect here is that segments are only persisted for warm roll when they are properly shutdown. The ",(0,i.mdx)("inlineCode",{parentName:"li"},"ShmManager")," has no limit on the number of segments it can manage. You can remove some segments and not attach to them on a warm roll. The ",(0,i.mdx)("inlineCode",{parentName:"li"},"ShmManager")," identifies the segments based on a name for the segment and the cache directory. The cache directory needs to be the same for ",(0,i.mdx)("inlineCode",{parentName:"li"},"ShmManager")," to be able to re-attach to the segments with the same name on a warm roll.")),(0,i.mdx)("h2",{id:"memory-allocator"},"Memory Allocator"),(0,i.mdx)("p",null,"As outlined above, the goal of the memory allocator is to provide support for a malloc like interface with pools and resizing. To achieve this, we allocate a chunk of memory initially and divide it into Slabs similar to a typical Slab based allocator. The memory allocator has a fixed size and if you want to grow the amount of memory, you need to instantiate a new one. However, with an existing memory allocator, we can resize the pools by growing a pool and shrinking others. In the future, when we move completely off of SYS-V apis, we will be able to grow or shrink the overall size of the memory allocator as well."),(0,i.mdx)("p",null,"The memory allocator is optimized for operation under full capacity. Once the cache grows to its full size, we evict existing allocations and repurpose them to make room for new ones without going to the memory allocator. Hence, the memory allocator is designed to operate under the assumption that once the cache grows, the allocator will mostly be full and out of memory. The memory allocator is primarily involved in moving slabs of memory across its pools and within each pool across different allocation sizes once the cache is full."),(0,i.mdx)("p",null,"Other than these, the ",(0,i.mdx)("inlineCode",{parentName:"p"},"MemoryAllocator")," also has support to compress the pointers to allocations. The current algorithm for pointer compression is optimized for unCompressing since we typically will be uncompressing pointers(accessing the item) much more than compressing them(when we allocate new items)."),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"Slab"),": Fixed sized chunks typically in the range of 4MB. The size of slab is defined at compile time and can not be changed without dropping the cache. Each slab at any point of time can be actively used or un-allocated or free. When it is actively used, the slab belongs to a particular pool and a particular allocation size within the pool. This is identified in the header for the slab and is synchronized by the lock in the ",(0,i.mdx)("inlineCode",{parentName:"li"},"AllocationClass")," and in ",(0,i.mdx)("inlineCode",{parentName:"li"},"MemoryPool")," when they acquire a slab. When a slab is un-allocated, it belongs to no pool or allocation class. When a slab is free, it can potentially belong to a pool or not. But does not belong to any allocation class. The Slab header also contains information about the allocation size for the slab and whether the slab is currently in the process of release."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"SlabAllocator"),": ",(0,i.mdx)("inlineCode",{parentName:"li"},"SlabAllocator")," manages the large chunk of memory and carves it into Slabs. The main api is to allocate and free slabs. It also provides support for pointer compression and maintains the apis for accessing the slab for a given memory location, its slab header etc. So getting the pool and allocation class information from any random memory location happens through the ",(0,i.mdx)("inlineCode",{parentName:"li"},"SlabAllocator"),"."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"AllocationClass"),":  An ",(0,i.mdx)("inlineCode",{parentName:"li"},"AllocationClass")," belongs to a Pool and corresponds to a particular allocation size within the pool. Each allocation class has a unique ",(0,i.mdx)("inlineCode",{parentName:"li"},"ClassId")," within the pool it belongs to. It can allocate allocations of its configured chunk size until it runs out of slabs. When it runs out of slabs, the allocations fail until more slabs are added into it."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"MemoryPool"),": Consists of a set of ",(0,i.mdx)("inlineCode",{parentName:"li"},"AllocationClass")," instances, one per configured allocation size for the pool. Each pool is identified by a name and has a unique PoolId.  When a given ",(0,i.mdx)("inlineCode",{parentName:"li"},"AllocationClass")," runs out of slabs, the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MemoryPool")," can fetch more slabs from the ",(0,i.mdx)("inlineCode",{parentName:"li"},"SlabAllocator")," and add it to the ",(0,i.mdx)("inlineCode",{parentName:"li"},"AllocationClass"),". The pool can allocate slabs from the ",(0,i.mdx)("inlineCode",{parentName:"li"},"SlabAllocator")," until it reaches the memory limit."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"MemoryPoolManager"),": Provides a simple interface to resolve a string name to ",(0,i.mdx)("inlineCode",{parentName:"li"},"PoolId")," and fetching the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MemoryPool")," corresponding to the name or ",(0,i.mdx)("inlineCode",{parentName:"li"},"PoolId"),". The ",(0,i.mdx)("inlineCode",{parentName:"li"},"MemoryPoolManager")," also handles creating or removing pools and keeping tabs on the over-all memory limit across the different pools. The sum of memory limits for all the pools should be strictly less than the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MemoryAllocator"),"'s total size."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"MemoryAllocator"),": Puts together all the above and provides the following APIs:",(0,i.mdx)("ol",{parentName:"li"},(0,i.mdx)("li",{parentName:"ol"},"allocate memory from a pool"),(0,i.mdx)("li",{parentName:"ol"},"release an allocated memory"),(0,i.mdx)("li",{parentName:"ol"},"adding or removing pools"),(0,i.mdx)("li",{parentName:"ol"},"slab release")))),(0,i.mdx)("p",null,"The main reason for doing Slab based memory allocation is to avoid fragmentation and enable us to rebalance or resize the cache in chunks of Slabs. By ensuring that a Slab can only contain allocations of one size, we can support a real simple implementation of compressing those pointers by just indexing the slab and the offset of allocation within the slab."),(0,i.mdx)("h2",{id:"cacheallocator"},"CacheAllocator"),(0,i.mdx)("p",null,(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," builds a cache using the ",(0,i.mdx)("inlineCode",{parentName:"p"},"MemoryAllocator"),".  It is a template class on ",(0,i.mdx)("inlineCode",{parentName:"p"},"MMType")," and ",(0,i.mdx)("inlineCode",{parentName:"p"},"AccessType")," and provides a malloc like API for allocating memory, that can be accessed through a key. The ",(0,i.mdx)("inlineCode",{parentName:"p"},"MMType")," and ",(0,i.mdx)("inlineCode",{parentName:"p"},"AccessType")," are template arguments to let ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," mix and match different implementations  of these based on application requirements. For instance, replace LRU with TimerWheels or replace ChainedHashTable with SomeFancyHashTable without having to re-implement the rest of the cache logic."),(0,i.mdx)("p",null,"The allocations done through ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," are called ",(0,i.mdx)("inlineCode",{parentName:"p"},"Item")," and they are refcounted. The users have a handle for the item when accessing it through any of the APIs and the ",(0,i.mdx)("inlineCode",{parentName:"p"},"Handle")," takes care of maintaining the refcount when it goes out of scope. Items are allocated out of an existing pool that must be created through the same ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," instance. ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," supports saving and restoring the cache through shared memory."),(0,i.mdx)("p",null,"The main APIs for CacheAllocator are:"),(0,i.mdx)("pre",null,(0,i.mdx)("code",{parentName:"pre",className:"language-cpp"},"WriteHandle allocate(PoolId pid, Key key, uint32_t size);\nReadHandle find(Key key);\n")),(0,i.mdx)("p",null,"The ",(0,i.mdx)("inlineCode",{parentName:"p"},"WriteHandle")," / ",(0,i.mdx)("inlineCode",{parentName:"p"},"ReadHandle")," is a valid handle to the ",(0,i.mdx)("inlineCode",{parentName:"p"},"Item")," and the caller holds a reference to the memory through the ",(0,i.mdx)("inlineCode",{parentName:"p"},"WriteHandle")," / ",(0,i.mdx)("inlineCode",{parentName:"p"},"ReadHandle"),". Its main interface is ",(0,i.mdx)("inlineCode",{parentName:"p"},"getMemory()")," which returns a ",(0,i.mdx)("inlineCode",{parentName:"p"},"void*")," for ",(0,i.mdx)("inlineCode",{parentName:"p"},"WriteHandle")," and ",(0,i.mdx)("inlineCode",{parentName:"p"},"const void*")," for ",(0,i.mdx)("inlineCode",{parentName:"p"},"ReadHandle")," that the user can use as a pointer to memory of requested size. It also supports API to access the key for the allocation."),(0,i.mdx)("p",null,"Some of the key abstractions in this are :"),(0,i.mdx)("ul",null,(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"MMTYpe/MMContainer"),": The ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMType")," is an implementation that helps with memory management of the Items. Items are initially allocated from the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MemoryAllocator")," and then added to a ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer"),". When the allocator no longer can make allocations, we recycle an existing Item through the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer"),". In that aspect, the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer")," basically figures out which Items are important and which ones can be evicted. ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer")," provides an eviction iterator that ",(0,i.mdx)("inlineCode",{parentName:"li"},"CacheAllocator")," uses to walk and find a suitable candidate for eviction. An example ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMType")," could be an LRU or FIFO or BucketBasedExpiration. Every ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMType")," implementation has an intrusive hook that is part of ",(0,i.mdx)("inlineCode",{parentName:"li"},"Item")," and acts as an intrusive container. The goal is to have all MMTypes have a standard interface that ",(0,i.mdx)("inlineCode",{parentName:"li"},"CacheAllocator")," can work with. There is one ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer")," per allocation size in every Pool. Hence when we want to allocate an Item of size X and are out of memory in the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MemoryAllocator"),", we recycle an existing Item from the corresponding MMContainer for the size X in the requested pool. The MMContainer's API is thread safe and protected by its own synchronization method(locks)."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"AccessType/AccessContainer"),": The ",(0,i.mdx)("inlineCode",{parentName:"li"},"AccessType")," controls how we access Items in the cache. So this is typically some implementation of HashTable. The AccessContainer is an intrusive hook based container and supports a standard interface that works with ",(0,i.mdx)("inlineCode",{parentName:"li"},"CacheAllocator"),". We currently have an implementation of ",(0,i.mdx)("inlineCode",{parentName:"li"},"ChainedHashTable")," that does chaining to resolve collisions.  Similar to the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer"),", the ",(0,i.mdx)("inlineCode",{parentName:"li"},"AccessContainer"),"'s API is also thread safe and protected by internal locks."),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"Refcount"),": Item's life cycle is managed through refcounts. We currently use 16 bits for refcount. Some of these bits are reserved for special purposes and the remaining is used for keeping track of the number of current ",(0,i.mdx)("inlineCode",{parentName:"li"},"Handles")," that are handed out of the cache. Typically, when the refcount drops to 0 during any time, we consider the Item to be free and release the memory for the Item back to the MemoryAllocator. The only exception to this is during eviction when we recycle the memory and make a new Item out of existing one without going through the free \u2192 alloc cycle. All refcounts operations are atomic and don't need any special synchronization by themselves. But interpreting the refcount would need appropriate synchronization in some cases. Some of the special bits in the 16 bit refcount are",(0,i.mdx)("ul",{parentName:"li"},(0,i.mdx)("li",{parentName:"ul"},"kLinked : This bit indicates that the ",(0,i.mdx)("inlineCode",{parentName:"li"},"Item")," is present in the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer")," and is active allocation. This bit is used by the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer"),"."),(0,i.mdx)("li",{parentName:"ul"},"kAccessible : This bit indicates that the ",(0,i.mdx)("inlineCode",{parentName:"li"},"Item")," is present in the ",(0,i.mdx)("inlineCode",{parentName:"li"},"AccessContainer")," and is accessible from the find method.  This bit is used by the ",(0,i.mdx)("inlineCode",{parentName:"li"},"AccessContainer"),"."),(0,i.mdx)("li",{parentName:"ul"},"kMoving : This bit indicates that the ",(0,i.mdx)("inlineCode",{parentName:"li"},"Item")," belongs to a ",(0,i.mdx)("inlineCode",{parentName:"li"},"Slab")," that is currently being released. Only the slab rebalancing thread can set or unset this bit. Typically, this bit is set conditionally when one of the other bits are set to deal with races during slab rebalancing. The purpose of the bit is to have the slab rebalancing thread take ownership of freeing the ",(0,i.mdx)("inlineCode",{parentName:"li"},"Item")," eventually without having to grab the locks for ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer")," or ",(0,i.mdx)("inlineCode",{parentName:"li"},"AccessContainer")," causing contention."))),(0,i.mdx)("li",{parentName:"ul"},(0,i.mdx)("strong",{parentName:"li"},"KAllocation"),": This is a simple wrapper that encapsulates a block of memory that holds the ",(0,i.mdx)("inlineCode",{parentName:"li"},"Key"),", size of the ",(0,i.mdx)("inlineCode",{parentName:"li"},"Key")," and the available memory in an ",(0,i.mdx)("inlineCode",{parentName:"li"},"Item"),".")),(0,i.mdx)("h3",{id:"slab-release"},"Slab Release"),(0,i.mdx)("p",null,(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," releases slabs for either resizing a ",(0,i.mdx)("inlineCode",{parentName:"p"},"Pool")," or rebalancing different ",(0,i.mdx)("inlineCode",{parentName:"p"},"MMContainers")," within a ",(0,i.mdx)("inlineCode",{parentName:"p"},"Pool")," to improve application specific metric(hit ratio, cpu etc). The latter is more relevant to a cache since the workloads change quite often when you have a cache server running for a long period of time. Depending on the application using Cachelib, we might want to react in a different way on how we want to do the rebalancing or resizing. So ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," supports providing a user defined implementation of ",(0,i.mdx)("inlineCode",{parentName:"p"},"RebalanceStrategy")," and ",(0,i.mdx)("inlineCode",{parentName:"p"},"ResizeStrategy"),". The goal of these are to help CacheAllocator pick a slab reassigning to a different allocation size that can help some application specific metric."),(0,i.mdx)("p",null,"To kick off a slab release, ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," talks to ",(0,i.mdx)("inlineCode",{parentName:"p"},"MemoryAllocator")," and starts a slab release through ",(0,i.mdx)("inlineCode",{parentName:"p"},"startSlabRelease")," call. This gives back a context that contains the slab we are targeting to release and the current active allocations in that slab at the point when the context was created. For the active allocations, ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," needs to ensure they are all freed back to the allocator before calling ",(0,i.mdx)("inlineCode",{parentName:"p"},"completeSlabRelease")," which finishes the slab release.  One of the challenges here is to ensure that we safely free the active allocations without any expensive synchronization. Since ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," by itself does not have any synchronization, this is a little hard. The active allocation can be in any of the possible states :"),(0,i.mdx)("ol",null,(0,i.mdx)("li",{parentName:"ol"},"Active allocation that is present in the ",(0,i.mdx)("inlineCode",{parentName:"li"},"MMContainer")," or ",(0,i.mdx)("inlineCode",{parentName:"li"},"AccessContainer"),"."),(0,i.mdx)("li",{parentName:"ol"},"Active allocation that is not present in either but has active references held by user"),(0,i.mdx)("li",{parentName:"ol"},"Active allocation that is present in one of these containers and in the process of being recycled."),(0,i.mdx)("li",{parentName:"ol"},"Active allocation that is in the process of being freed to the allocator.")),(0,i.mdx)("p",null,"Typically, when ",(0,i.mdx)("inlineCode",{parentName:"p"},"CacheAllocator")," is dealing with an ",(0,i.mdx)("inlineCode",{parentName:"p"},"Item"),", it synchronizes access to the ",(0,i.mdx)("inlineCode",{parentName:"p"},"Item")," through the ",(0,i.mdx)("inlineCode",{parentName:"p"},"MMContainer")," or ",(0,i.mdx)("inlineCode",{parentName:"p"},"AccessContainer"),". We grab references to the Item upon these synchronization. During slab release, one possible option is to wait until all allocations get freed through some other regular cache process. But this could take a while. So the slab rebalancer tries to evict these active Items and free them manually. But, we need to ensure that we don't race with any other threads and double free any active allocations. To help with this across several states that the ",(0,i.mdx)("inlineCode",{parentName:"p"},"Item")," can be in, the rebalancing thread uses a special bit in the refcount to indicate that the ",(0,i.mdx)("inlineCode",{parentName:"p"},"Item")," is being moved. We set this bit conditionally when one of the other bits is set. This ensures that when the ",(0,i.mdx)("inlineCode",{parentName:"p"},"Item")," is in any valid state and we set this bit, it can not be freed by any other process. We also ensure that during the eviction, we don't touch Items that are in moving state. For these Items which have the moving bit set, the slab rebalancing thread then tries to remove them from the cache and wait for the refcounts to drop before calling free on them."),(0,i.mdx)("h3",{id:"pool-rebalancer"},"Pool rebalancer"),(0,i.mdx)("p",null,"The ",(0,i.mdx)("inlineCode",{parentName:"p"},"PoolRebalancer")," is an asynchronous thread that rebalances the allocation of slabs to different allocation sizes within a pool. It uses a user provided RebalanceMetric to determine which allocation sizes need more memory and which ones don't and executes a slab release periodically when it makes sense to rebalance. Pools are rebalanced only after they have grown to their full sizes."),(0,i.mdx)("h3",{id:"pool-resizer"},"Pool resizer"),(0,i.mdx)("p",null,"The ",(0,i.mdx)("inlineCode",{parentName:"p"},"PoolResizer")," ensures that the pools get sized down when they are over their limit. This typically happens when we size down a pool and the current size of the pool is more than its limit. The resizer watches for these pools and triggers a slab release from the pools that releases the slab back to the MemoryAllocator for use in other pools. This is asynchronous and pools slowly grow in size and shrink as the resizer works through the slabs."))}p.isMDXComponent=!0}}]);