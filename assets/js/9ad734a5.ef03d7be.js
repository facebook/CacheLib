"use strict";(self.webpackChunkcachelib=self.webpackChunkcachelib||[]).push([[673],{15680:(e,t,a)=>{a.r(t),a.d(t,{MDXContext:()=>m,MDXProvider:()=>h,mdx:()=>b,useMDXComponents:()=>s,withMDXComponents:()=>d});var n=a(96540);function i(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function r(){return r=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var a=arguments[t];for(var n in a)Object.prototype.hasOwnProperty.call(a,n)&&(e[n]=a[n])}return e},r.apply(this,arguments)}function o(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?o(Object(a),!0).forEach((function(t){i(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):o(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function c(e,t){if(null==e)return{};var a,n,i=function(e,t){if(null==e)return{};var a,n,i={},r=Object.keys(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||(i[a]=e[a]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)a=r[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(i[a]=e[a])}return i}var m=n.createContext({}),d=function(e){return function(t){var a=s(t.components);return n.createElement(e,r({},t,{components:a}))}},s=function(e){var t=n.useContext(m),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},h=function(e){var t=s(e.components);return n.createElement(m.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,i=e.mdxType,r=e.originalType,o=e.parentName,m=c(e,["components","mdxType","originalType","parentName"]),d=s(a),h=i,u=d["".concat(o,".").concat(h)]||d[h]||p[h]||r;return a?n.createElement(u,l(l({ref:t},m),{},{components:a})):n.createElement(u,l({ref:t},m))}));function b(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=a.length,o=new Array(r);o[0]=u;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var m=2;m<r;m++)o[m]=a[m];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},7585:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>l,metadata:()=>m,toc:()=>s});var n=a(9668),i=a(21367),r=(a(96540),a(15680)),o=["components"],l={id:"Object_Cache_Architecture_Guide",title:"Object-Cache Design"},c=void 0,m={unversionedId:"facebook/Object_Cache/Object_Cache_Architecture_Guide",id:"facebook/Object_Cache/Object_Cache_Architecture_Guide",title:"Object-Cache Design",description:"Introduction",source:"@site/docs/facebook/Object_Cache/object_cache_architecture_guide.md",sourceDirName:"facebook/Object_Cache",slug:"/facebook/Object_Cache/Object_Cache_Architecture_Guide",permalink:"/docs/facebook/Object_Cache/Object_Cache_Architecture_Guide",draft:!1,editUrl:"https://github.com/facebook/CacheLib/edit/main/website/docs/facebook/Object_Cache/object_cache_architecture_guide.md",tags:[],version:"current",frontMatter:{id:"Object_Cache_Architecture_Guide",title:"Object-Cache Design"}},d={},s=[{value:"Introduction",id:"introduction",level:2},{value:"Design Details",id:"design-details",level:2},{value:"CacheLib allocated memory",id:"cachelib-allocated-memory",level:3},{value:"allocation size",id:"allocation-size",level:4},{value:"cache size",id:"cache-size",level:4},{value:"sharding",id:"sharding",level:4},{value:"Heap memory",id:"heap-memory",level:3},{value:"size-awareness: tracking and controlling heap memory usage",id:"size-awareness-tracking-and-controlling-heap-memory-usage",level:4},{value:"Object lifetime",id:"object-lifetime",level:3}],h={toc:s};function p(e){var t=e.components,a=(0,i.A)(e,o);return(0,r.mdx)("wrapper",(0,n.A)({},h,a,{components:t,mdxType:"MDXLayout"}),(0,r.mdx)("h2",{id:"introduction"},"Introduction"),(0,r.mdx)("p",null,"Object-Cache enables CacheLib users to cache complex C++ objects natively."),(0,r.mdx)("p",null,"Before Object-Cache, users who need to cache complex/non-POD C++ structs are forced to choose between the following 2 ways:"),(0,r.mdx)("ul",null,(0,r.mdx)("li",{parentName:"ul"},"(common) serializing/deserializing objects on each write/read -> which is CPU-expensive and sometimes hard to implement"),(0,r.mdx)("li",{parentName:"ul"},"re-implementing the C++ data structures to work with CacheLib memory allocator (e.g. ",(0,r.mdx)("a",{parentName:"li",href:"/docs/Cache_Library_User_Guides/Structured_Cache#map"},"cachelib map"),") -> which is difficult to develop and maintain over time")),(0,r.mdx)("p",null,"With Object-Cache, users are able to cache complex C++ objects without going through either of the above two pains. Once the object has been allocated on the heap (e.g. via ",(0,r.mdx)("inlineCode",{parentName:"p"},"std::make_unique"),"), Object-Cache tracks the ",(0,r.mdx)("strong",{parentName:"p"},"object pointer")," instead of maintaining a full copy of the object. In this case, each Object-Cache lookup is almost as efficient as a pointer access; each Object-Cache allocation only copies the object pointer plus some metadata (note that user is responsible for the heap allocation of the object)."),(0,r.mdx)("p",null,"This approach also implies that Object-Cache achieves CPU efficiency and implementation simplification by sacrificing memory efficiency: an unserialized object can be much larger than its serialized version. Therefore, when considering if Object-Cache v.s. the regular CacheLib is a better fit, you should always evaluate your system's constraints (CPU, memory, etc.) and the amplification rate (i.e. unserialized object size : serialized object size). A general rule is only considering Object-Cache when you have to cache non-POD C++ objects and your system has enough memory headroom to store objects in an unserialized form."),(0,r.mdx)("h2",{id:"design-details"},"Design Details"),(0,r.mdx)("p",null,"Object-Cache is built on top of the regular CacheLib. We can view its memory composition as two parts: CacheLib allocated memory (RAM-only) and heap memory. Total Object-Cache Memory = CacheLib allocated memory + heap memory."),(0,r.mdx)("table",null,(0,r.mdx)("thead",{parentName:"table"},(0,r.mdx)("tr",{parentName:"thead"},(0,r.mdx)("th",{parentName:"tr",align:null}),(0,r.mdx)("th",{parentName:"tr",align:null},"Allocator"),(0,r.mdx)("th",{parentName:"tr",align:null},"Data"),(0,r.mdx)("th",{parentName:"tr",align:null},"Data type"))),(0,r.mdx)("tbody",{parentName:"table"},(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},"CacheLib memory"),(0,r.mdx)("td",{parentName:"tr",align:null},"CacheLib MemoryAllocator"),(0,r.mdx)("td",{parentName:"tr",align:null},"object pointer, key, metadata"),(0,r.mdx)("td",{parentName:"tr",align:null},"only allow POD")),(0,r.mdx)("tr",{parentName:"tbody"},(0,r.mdx)("td",{parentName:"tr",align:null},"heap memory"),(0,r.mdx)("td",{parentName:"tr",align:null},"C++ heap memory allocator, e.g. jemalloc"),(0,r.mdx)("td",{parentName:"tr",align:null},"actual object"),(0,r.mdx)("td",{parentName:"tr",align:null},"any type of C++ structs")))),(0,r.mdx)("h3",{id:"cachelib-allocated-memory"},"CacheLib allocated memory"),(0,r.mdx)("p",null,"CacheLib allocated memory contains ",(0,r.mdx)("inlineCode",{parentName:"p"},"ObjectCacheItem"),", ",(0,r.mdx)("inlineCode",{parentName:"p"},"CacheItem")," and the cache key."),(0,r.mdx)("ul",null,(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("inlineCode",{parentName:"li"},"ObjectCacheItem"),": object pointer and the amount of heap memory required by the object")),(0,r.mdx)("pre",null,(0,r.mdx)("code",{parentName:"pre",className:"language-cpp"},"struct ObjectCacheItem {\n  uintptr_t objectPtr; // raw pointer of the object\n  size_t objectSize; // used for size-awareness which will be explained later\n};\n")),(0,r.mdx)("ul",null,(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("inlineCode",{parentName:"li"},"CacheItem"),": regular CacheLib Item, storing regular CacheLib required metadata"),(0,r.mdx)("li",{parentName:"ul"},"cache key: key to look up the object")),(0,r.mdx)("h4",{id:"allocation-size"},"allocation size"),(0,r.mdx)("p",null,(0,r.mdx)("inlineCode",{parentName:"p"},"ObjectCacheItem"),' is essentially the "value" stored in the CacheLib cache. For each CacheLib allocation, the required size can be calculated using the following formula:'),(0,r.mdx)("pre",null,(0,r.mdx)("code",{parentName:"pre",className:"language-cpp"},"required size of a CacheLib allocation\n= value.size() + key.size() + sizeof(CacheItem)\n= sizeof(ObjectCacheItem) + key.size() + sizeof(CacheItem) +\n= 16 bytes + key.size() + 32 bytes\n= 48 bytes + key.size()\n")),(0,r.mdx)("p",null,"Therefore, a single allocation class can be used for all CacheLib allocations."),(0,r.mdx)("pre",null,(0,r.mdx)("code",{parentName:"pre",className:"language-cpp"},"defaultAllocSize = (48 bytes + maxKeySize) align to 8 bytes\n")),(0,r.mdx)("p",null,(0,r.mdx)("em",{parentName:"p"},"The maximum key size here is configurable (default to be the maximum value 255-byte).")),(0,r.mdx)("h4",{id:"cache-size"},"cache size"),(0,r.mdx)("p",null,"CacheLib allocated memory is pre-allocated memory. To estimate the total size of this part of memory, we ask the user to specify ",(0,r.mdx)("inlineCode",{parentName:"p"},"entriesLimit"),": the maximum number of objects stored in Object-Cache. Then the CacheLib cache size is approximate to ",(0,r.mdx)("inlineCode",{parentName:"p"},"entriesLimit * defaultAllocSize"),"."),(0,r.mdx)("pre",null,(0,r.mdx)("code",{parentName:"pre",className:"language-cpp"},"CacheLib cache size ~= entriesLimit * defaultAllocSize\n")),(0,r.mdx)("h4",{id:"sharding"},"sharding"),(0,r.mdx)("p",null,'CacheLib allocated memory can still be split into multiple pools. While here the "pool" functions more like a "shard": it is no longer to partition different workloads but to improve insertion concurrency. All pools have the same pool size and use the same allocation size (i.e. ',(0,r.mdx)("inlineCode",{parentName:"p"},"defaultAllocSize"),"). To determine the pool id of an incoming object, we use a hash-based sharding mechanism."),(0,r.mdx)("pre",null,(0,r.mdx)("code",{parentName:"pre",className:"language-cpp"},"auto hash = cachelib::MurmurHash2{}(key.data(), key.size());\npoolId = static_cast<PoolId>(hash % numShards);\n")),(0,r.mdx)("h3",{id:"heap-memory"},"Heap memory"),(0,r.mdx)("p",null,"The actual objects are allocated on the heap. If jemalloc is the default allocator, it will be allocated via jemalloc. A normal way to do such allocation is calling ",(0,r.mdx)("inlineCode",{parentName:"p"},"std::make_unique<ObjectType>(...)"),"."),(0,r.mdx)("h4",{id:"size-awareness-tracking-and-controlling-heap-memory-usage"},"size-awareness: tracking and controlling heap memory usage"),(0,r.mdx)("p",null,"Obviously, this part of memory is dynamic allocated and not managed by CacheLib. Tracking it won't be as precise and straightforward as tracking memory allocated by CacheLib. Considering there is already a limit for #objects, user may not have to track the memory footprint here. However, if the object sizes can vary within a wide range, only tracking #objects is not enough. In this case, we introduced ",(0,r.mdx)("strong",{parentName:"p"},"size-awareness")," feature to track and control the heap memory usage within certain limit. Note that this feature assumes ",(0,r.mdx)("strong",{parentName:"p"},"jemalloc")," as the allocator."),(0,r.mdx)("ol",null,(0,r.mdx)("li",{parentName:"ol"},"calculate the amount of net allocated heap memory when constructing the object",(0,r.mdx)("ul",{parentName:"li"},(0,r.mdx)("li",{parentName:"ul"},"this can be done by ",(0,r.mdx)("a",{parentName:"li",href:"/docs/facebook/Object_Cache/Object_Cache_User_Guide#how-to-calculate-object-size"},"jemalloc util functions")))),(0,r.mdx)("li",{parentName:"ol"},"pass the size information to Object-Cache so that it can track each object's size and the total size",(0,r.mdx)("ul",{parentName:"li"},(0,r.mdx)("li",{parentName:"ul"},"insertion: set ",(0,r.mdx)("inlineCode",{parentName:"li"},"ObjectCacheItem::objectSize")," and increase ",(0,r.mdx)("inlineCode",{parentName:"li"},"totalObjectSize")," by ",(0,r.mdx)("inlineCode",{parentName:"li"},"ObjectCacheItem::objectSize")),(0,r.mdx)("li",{parentName:"ul"},"mutation: update ",(0,r.mdx)("inlineCode",{parentName:"li"},"ObjectCacheItem::objectSize")," by delta and update ",(0,r.mdx)("inlineCode",{parentName:"li"},"totalObjectSize")," by the same delta"),(0,r.mdx)("li",{parentName:"ul"},"remove and eviction: decrease ",(0,r.mdx)("inlineCode",{parentName:"li"},"totalObjectSize")," by ",(0,r.mdx)("inlineCode",{parentName:"li"},"ObjectCacheItem::objectSize")))),(0,r.mdx)("li",{parentName:"ol"},"run a background thread to peridocially adjust the current ",(0,r.mdx)("inlineCode",{parentName:"li"},"entriesLimit")," based on the current ",(0,r.mdx)("inlineCode",{parentName:"li"},"totalObjectSize"),", and eviction will be triggered when the limit is reached",(0,r.mdx)("ul",{parentName:"li"},(0,r.mdx)("li",{parentName:"ul"},(0,r.mdx)("strong",{parentName:"li"},"size-controller")," is the background thread for step 3. It starts when Object-Cache is created. Since then it will periodically check ",(0,r.mdx)("inlineCode",{parentName:"li"},"totalObjectSize")," and adjust the ",(0,r.mdx)("inlineCode",{parentName:"li"},"currEntriesLimit")," accordingly:",(0,r.mdx)("ul",{parentName:"li"},(0,r.mdx)("li",{parentName:"ul"},"initial value of ",(0,r.mdx)("inlineCode",{parentName:"li"},"currEntriesLimit")," is ",(0,r.mdx)("inlineCode",{parentName:"li"},"config.entriesLimit")),(0,r.mdx)("li",{parentName:"ul"},"allocating placeholders to decrease ",(0,r.mdx)("inlineCode",{parentName:"li"},"currEntriesLimit")," and trigger eviction"),(0,r.mdx)("li",{parentName:"ul"},"deallocating placeholders to increase ",(0,r.mdx)("inlineCode",{parentName:"li"},"currEntriesLimit"))))))),(0,r.mdx)("pre",null,(0,r.mdx)("code",{parentName:"pre",className:"language-cpp"},"// get new entriesLimit\naverageObjSize = current totalObjSize / current #objects;\nnewEntriesLimit = min(configured entriesLimit, configured heapSizeLimit / averageObjSize)\n\n// adjust placeholders\nif (newEntriesLimit < currEntriesLimit) {\n   if (currentEntriesNum reaches newEntriesLimit) {\n     // allocate more placeholders to shrink the cache\n     allocate (currEntriesLimit - newEntriesLimit) more placeholders\n   } else {\n     do nothing\n   }\n} else if (newEntriesLimit > currEntriesLimit) {\n   if (currentEntriesNum reaches currEntriesLimit) {\n     // deallocate placeholders to expand the cache\n     deallocate (newEntriesLimit - currEntriesLimit) more placeholders\n   } else {\n     do nothing\n   }\n}\n\ncurrEntriesLimit = newEntriesLimit\n\n")),(0,r.mdx)("h3",{id:"object-lifetime"},"Object lifetime"),(0,r.mdx)("p",null,"You might ask since Object-Cache only stores the pointers, how to ensure the object can stay on the heap until Object-Cache no longer need to access the object? Actually, the control of the object's lifetime will be transferred to Object-Cache once it is inserted into Object-Cache."),(0,r.mdx)("ol",null,(0,r.mdx)("li",{parentName:"ol"},(0,r.mdx)("inlineCode",{parentName:"li"},"std::make_unique")," to allocate the object on the heap"),(0,r.mdx)("li",{parentName:"ol"},"request an Object-Cache insertion"),(0,r.mdx)("li",{parentName:"ol"},"request a CacheLib allocation -> returns a CacheLib Item's ",(0,r.mdx)("a",{parentName:"li",href:"/docs/Cache_Library_User_Guides/Item_and_Handle"},(0,r.mdx)("inlineCode",{parentName:"a"},"Handle"))),(0,r.mdx)("li",{parentName:"ol"},"create an ",(0,r.mdx)("inlineCode",{parentName:"li"},"ObjectCacheItem")," by copying the object pointer (",(0,r.mdx)("inlineCode",{parentName:"li"},"object.get()"),")"),(0,r.mdx)("li",{parentName:"ol"},"copy ",(0,r.mdx)("inlineCode",{parentName:"li"},"ObjectCacheItem")," to the ",(0,r.mdx)("inlineCode",{parentName:"li"},"Handle")),(0,r.mdx)("li",{parentName:"ol"},"insert the ",(0,r.mdx)("inlineCode",{parentName:"li"},"Handle")," to CacheLib"),(0,r.mdx)("li",{parentName:"ol"},"after a successful insertion, the ownership of the object will be transferred to Object-Cache; since then, Object-Cache manages the reference counts of the object internally and is responsible for destroying the object when it's evicted or removed")),(0,r.mdx)("p",null,"Object-Cache lookup returns a shared pointer using a custom deleter. This custom deleter holds a ",(0,r.mdx)("inlineCode",{parentName:"p"},"Handle")," generated via CacheLib find API and will only release this ",(0,r.mdx)("inlineCode",{parentName:"p"},"Handle")," when the shared pointer reaches 0 reference count. This aligns with the existing CacheLib design that a ",(0,r.mdx)("inlineCode",{parentName:"p"},"Handle")," is similar to a ",(0,r.mdx)("inlineCode",{parentName:"p"},"std::shared_ptr"),". Only when all handles are released which will trigger the item destructor, we should destroy the object memory."))}p.isMDXComponent=!0}}]);