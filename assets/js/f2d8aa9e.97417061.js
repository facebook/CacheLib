"use strict";(self.webpackChunkcachelib=self.webpackChunkcachelib||[]).push([[4281],{3905:function(e,t,i){i.r(t),i.d(t,{MDXContext:function(){return l},MDXProvider:function(){return u},mdx:function(){return f},useMDXComponents:function(){return m},withMDXComponents:function(){return h}});var n=i(67294);function a(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function o(){return o=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var i=arguments[t];for(var n in i)Object.prototype.hasOwnProperty.call(i,n)&&(e[n]=i[n])}return e},o.apply(this,arguments)}function r(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function s(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?r(Object(i),!0).forEach((function(t){a(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):r(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function c(e,t){if(null==e)return{};var i,n,a=function(e,t){if(null==e)return{};var i,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||(a[i]=e[i]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(a[i]=e[i])}return a}var l=n.createContext({}),h=function(e){return function(t){var i=m(t.components);return n.createElement(e,o({},t,{components:i}))}},m=function(e){var t=n.useContext(l),i=t;return e&&(i="function"==typeof e?e(t):s(s({},t),e)),i},u=function(e){var t=m(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var i=e.components,a=e.mdxType,o=e.originalType,r=e.parentName,l=c(e,["components","mdxType","originalType","parentName"]),h=m(i),u=a,p=h["".concat(r,".").concat(u)]||h[u]||d[u]||o;return i?n.createElement(p,s(s({ref:t},l),{},{components:i})):n.createElement(p,s({ref:t},l))}));function f(e,t){var i=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=i.length,r=new Array(o);r[0]=p;var s={};for(var c in t)hasOwnProperty.call(t,c)&&(s[c]=t[c]);s.originalType=e,s.mdxType="string"==typeof e?e:a,r[1]=s;for(var l=2;l<o;l++)r[l]=i[l];return n.createElement.apply(null,r)}return n.createElement.apply(null,i)}p.displayName="MDXCreateElement"},65463:function(e,t,i){i.r(t),i.d(t,{contentTitle:function(){return c},default:function(){return u},frontMatter:function(){return s},metadata:function(){return l},toc:function(){return h}});var n=i(87462),a=i(63366),o=(i(67294),i(3905)),r=["components"],s={id:"WSA_helpful_definitions",title:"Helpful Definitions"},c=void 0,l={unversionedId:"facebook/Working_Set_Analysis/WSA_helpful_definitions",id:"facebook/Working_Set_Analysis/WSA_helpful_definitions",isDocsHomePage:!1,title:"Helpful Definitions",description:"Context",source:"@site/docs/facebook/Working_Set_Analysis/WSA_helpful_definitions.md",sourceDirName:"facebook/Working_Set_Analysis",slug:"/facebook/Working_Set_Analysis/WSA_helpful_definitions",permalink:"/docs/facebook/Working_Set_Analysis/WSA_helpful_definitions",editUrl:"https://github.com/facebook/CacheLib/edit/main/website/docs/facebook/Working_Set_Analysis/WSA_helpful_definitions.md",tags:[],version:"current",frontMatter:{id:"WSA_helpful_definitions",title:"Helpful Definitions"}},h=[{value:"Context",id:"context",children:[]},{value:"Cache Definitions",id:"cache-definitions",children:[]},{value:"Working Set Analysis Definitions",id:"working-set-analysis-definitions",children:[]}],m={toc:h};function u(e){var t=e.components,i=(0,a.Z)(e,r);return(0,o.mdx)("wrapper",(0,n.Z)({},m,i,{components:t,mdxType:"MDXLayout"}),(0,o.mdx)("h3",{id:"context"},"Context"),(0,o.mdx)("p",null,"This page is intended to provide definitions for people who may be unfamiliar with caches or unfamiliar with some specific vocabulary used when discussing Working Set Analysis."),(0,o.mdx)("h3",{id:"cache-definitions"},"Cache Definitions"),(0,o.mdx)("p",null,'The cache is an important element of most data storage architectures. Caches store data locally so that future requests for the same data can be served faster. Caches are typically deployed "in front" of a slower data store. When requested data exists and can be served from the cache, we call that request a ',(0,o.mdx)("strong",{parentName:"p"},"cache hit")," or simply a ",(0,o.mdx)("strong",{parentName:"p"},'"hit"'),". Conversely, requests which cannot be served from the cache are referred to as ",(0,o.mdx)("strong",{parentName:"p"},'"misses"'),"."),(0,o.mdx)("p",null,"Because a cache itself has finite size, the software component of the cache must decide what to write to cache and what to evict from the cache when space is exhausted. There are many well-known ",(0,o.mdx)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Cache_(computing)#Writing_policies"},"writing policies")," and ",(0,o.mdx)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Cache_replacement_policies"},"eviction policies")," for handling these cases."),(0,o.mdx)("p",null,'Working Set Analysis focuses primarily on C++ caches at Facebook which are largely built upon Cachelib. Cachelib is a highly performant library which provides a powerful and flexible cache API. Cachelib caches always store data locally in DRAM; some caches also utilize flash storage to increase the overall capacity of the cache. Several teams at Facebook also use systems of caches which support each other. For example, TAO uses DRAM-only caches called "followers" whose misses fall back to DRAM+Flash caches called "leaders"; misses from TAO leaders hit MySQL.'),(0,o.mdx)("p",null,"The primary goal of any caching + storage system is to achieve low-latency accesses with as lean of a deployment as possible. The Working Set Analysis toolkit analyzes cache traffic to provide insights into how to best optimize several competing objectives:"),(0,o.mdx)("ul",null,(0,o.mdx)("li",{parentName:"ul"},"Overall latency (related to overall ",(0,o.mdx)("strong",{parentName:"li"},"hit rate"),"; hits / (hits + misses))"),(0,o.mdx)("li",{parentName:"ul"},"Overall size"),(0,o.mdx)("li",{parentName:"ul"},"Flash I/O (if applicable)")),(0,o.mdx)("h3",{id:"working-set-analysis-definitions"},"Working Set Analysis Definitions"),(0,o.mdx)("p",null,"These are the terms we most commonly use when talking about Working Set Analysis."),(0,o.mdx)("h4",{id:"request-trace"},"Request Trace"),(0,o.mdx)("p",null,"A request trace is a time-ordered series of cache operations (SET, GET, DELETE). Every request/operation is attributable to a single ",(0,o.mdx)("strong",{parentName:"p"},"object")," identified by a ",(0,o.mdx)("strong",{parentName:"p"},"key")," (if applicable, we interpret any multi-object operations as multiple operations for single objects). We most often measure the request trace with respect to a server (i.e. the host/process that is receiving the request) as this has the most clear relationship with cache behavior. However, any time-ordered series of requests can define a request trace. We could, for example, consider all requests originating from a specific client to define a request trace."),(0,o.mdx)("p",null,"The terms \u201crequest\u201d and \u201coperation\u201d are mostly interchangeable for our purposes. We typically use \u201crequest\u201d to refer to the event that actually gets logged to our raw dataset, which includes both the cache operation as well as some extra metadata."),(0,o.mdx)("h4",{id:"working-set"},"Working Set"),(0,o.mdx)("p",null,"The \u201cworking set\u201d of a ",(0,o.mdx)("strong",{parentName:"p"},"request trace")," is the set of distinct objects referenced within a given time window."),(0,o.mdx)("p",null,"Note that the working set is an attribute of the ",(0,o.mdx)("strong",{parentName:"p"},"request trace"),' not an attribute of the cache itself. This leads to results that are sometimes unintuitive e.g. a SET operation will add an item to the working set, but a DELETE operation will not remove that same item from the working set. Note as well that we use the term "working set" here because of the natural parallels with mathematical set arithmetic. We often talk about taking the intersection or union of multiple working sets to derive insights about the traffic.'),(0,o.mdx)("p",null,"Some metrics we can define on the working set are:\nThe working set size (measured either in the number of distinct objects, or in the total size of the distinct objects in bytes)\nChurn and emergence\nGiven timestamps t1 < t2 < t3 < t4, we can define two ",(0,o.mdx)("strong",{parentName:"p"},"working sets")," as A(t1, t2) and B(t3, t4). ",(0,o.mdx)("strong",{parentName:"p"},"Churn")," is a measure of the objects appearing in A but not in B. ",(0,o.mdx)("strong",{parentName:"p"},"Emergence")," is a measure of the objects appearing in B but not in A."),(0,o.mdx)("h4",{id:"tenant"},"Tenant"),(0,o.mdx)("p",null,"A unifying concept for cache \u201cobject type\u201d (e.g. \u201ckey prefix\u201d for Memcache, \u201cfbtype\u201d for TAO fbobj, \u201ccdn_object_type\u201d for CDN, etc.). Functionally, we can define tenants as ",(0,o.mdx)("strong",{parentName:"p"},"any disjoint sets of the objects in the cache"),". We could, for example, define two tenants \u201cbig\u201d and \u201csmall\u201c, \u201dold\u201c and \u201dnew\u201c, \u201dbursty\u201c and \u201dregular\u201c, etc."),(0,o.mdx)("h4",{id:"miss-cost"},"Miss Cost"),(0,o.mdx)("p",null,"Miss costs are incurred when an item is requested from the cache but not found. For each cache miss there is a CPU cost on the server, a CPU cost on the client, latency costs, and typically a backend cost to refill the item in cache. The relative importance of each of these costs varies depending on the operational environment. Due to the inherent complexity of computing an absolute miss cost, we usually choose to work with relative miss costs that can vary by tenant or by request. In the simplest case, we can specify a relative miss cost of 1 for all requests, in which case \u201cminimizing miss cost\u201d corresponds to a hit rate maximization."),(0,o.mdx)("h4",{id:"hit-cost"},"Hit Cost"),(0,o.mdx)("p",null,"The cache itself intrinsically incurs costs for each operation. Pointers are updated, copies are made, locks are acquired, etc. etc. etc. These costs can add up to be non-trivial especially if the service itself has some non-caching functionality and needs to save CPU to do other work. For now we intentionally assume hit costs are significantly less than miss cost, and are happy to trade one for the other within Cachelib\u2019s scope. However there are opportunities for hit cost savings in the layers \u201cabove\u201d the cache e.g. with intelligent load shedding."),(0,o.mdx)("h4",{id:"tta"},"TTA"),(0,o.mdx)("p",null,"TTA (time-to-access) evictions are a pseudo-isolation mechanism that approximates the effect of each tenant getting its own queue. TTA allows much finer granularity than possible with cache pools. Using TTA with static tenant assignment approximates a pseudo-LRU for each tenant. Assigning multiple contextual TTA thresholds (e.g. a \u201cbursty\u201d threshold and a non-bursty threshold) for each tenant can approximate more complex eviction policies like 2Q (note: we do not currently have this capability available). TTA is currently enforced as a short-circuited eviction event that ensures items are expired as soon as their time-since-last-access threshold has elapsed."),(0,o.mdx)("h4",{id:"item-lifetime"},"Item Lifetime"),(0,o.mdx)("p",null,"We define an item\u2019s lifetime as how long it is kept in cache until its eviction. We can frame most of our existing heuristics in the form of \u201cextending to an item\u2019s lifetime\u201d, or measuring \u201chit density over an item\u2019s lifetime\u201d. For example, LRU with an eviction age of 10 minutes will give 10 minutes additional lifetime to each item on access. A ML policy will admit an item that is projected to receive more than 5 accesses in the next hour. ",(0,o.mdx)("strong",{parentName:"p"},"Modeling a policy around lifetime is computationally simple, and it gives us a shared framework to compare and contrast different heuristics"),". However, lifetime modeling becomes inaccurate with increasing variation of the lifetime of each individual tenant. (E.g. a flash cache with varying write rate throughout the day and we try to predict # of hits over the lifetime of an item)."),(0,o.mdx)("h4",{id:"item-value"},"Item Value"),(0,o.mdx)("p",null,"We define an item\u2019s value as its relative rank in relation to other items in the cache. We can view the role of the cache as maintaining an item ranking such that the lowest value items are replaced whenever a new item is added to the queue. With this framing in mind, all eviction policies implicitly assume a model for an item\u2019s value (LRU models item value as monotonically decreasing with age, LFU models item value as increasing with the number of accesses in an observed window, policies like GDSF explicitly maintain a priority queue with a \u201cvalue\u201d score). ",(0,o.mdx)("strong",{parentName:"p"},"Ranking allows us to model policies that inherently act on features that vary significantly over short period of time"),". (e.g. the LRU length and eviction age as traffic peaks and troughs). On the other hand, with ranking we require more complex simulation to compare and contrast different heuristics."))}u.isMDXComponent=!0}}]);