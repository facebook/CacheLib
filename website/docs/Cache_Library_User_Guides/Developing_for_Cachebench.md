---
id: Developing_for_Cachebench
title: Developing for Cachebench
---

This guide will explain how CacheBench is structured and how to add new configs or build features for it.

# Anatomy Of CacheBench

CacheBench is a benchmark suite that can read a workload configuration file, simulate cache behavior as stipulated in the config, and produce performance summary for the simulated cache. Results include metrics such as hit rate, evictions, write rate to flash cache, latency, etc. The workload configs can be hand-written by a human, produced by a workload analyzer, or backed by raw production cachelib traces. The main customization points into CacheBench are through writing workload configs or custom workload generators.
![](cachebench.png)

# Write A New Test Config

## A Simple Test Config

The following config sets up a basic cache instance with two DRAM cache pools and also sets up Navy and runs it in a DRAM-backed mode (useful for testing). The test config itself specifies the number of operations per threads, number of threads, number of keys, and then proceeds to describe the distribution of its key and value sizes and the distribution of the operations. It’s an example of a simple config that is usually written by a person for the purpose of adding a new integration test or a simple benchmark for a particular feature. Configs in this manner are not meant for representing real life workloads and used for performance measurements. For reference on what each option means, please refer to these files.

* Cache Config: https://fburl.com/zrul0wof
* Test Config: https://fburl.com/8x5318bv

```
{
  "cache_config" : {
    "cacheSizeMB" : 128,
    "poolRebalanceIntervalSec" : 1,
    "moveOnSlabRelease" : false,

    "numPools" : 2,
    "poolSizes" : [0.3, 0.7],

    "dipperSizeMB" : 512,
    "dipperBackend" : "navy_dipper",
    "enableChainedItem" : true,
    "dipperUseDirectIO": false,
    "dipperFilePath" : "/dev/shm/cachebench"
  },
  "test_config" :
    {
      "prepopulateCache" : true,

      "numOps" : 100000,
      "numThreads" : 16,
      "numKeys" : 100000,
      "distribution" :  "range",

      "keySizeRange" : [1, 8, 64],
      "keySizeRangeProbability" : [0.3, 0.7],

      "valSizeRange" : [256, 1024, 4096],
      "valSizeRangeProbability" : [0.2, 0.8],

      "chainedItemLengthRange" : [1, 2, 4, 32],
      "chainedItemLengthRangeProbability" : [0.8, 0.18, 0.02],

      "chainedItemValSizeRange" : [1, 128, 256, 1024, 4096, 20480],
      "chainedItemValSizeRangeProbability" : [0.1, 0.1, 0.2, 0.3, 0.3],

      "getRatio" : 0.5,
      "setRatio" : 0.3,
      "addChainedRatio" : 0.2,
      "keyPoolDistribution": [0.5, 0.5],
      "opPoolDistribution" : [0.5, 0.5]
    }
}
```
The value size will be changed to max(valSize, sizeof(CacheValue)) when allocate in the cache for cachebench. If the size distribution is important to the test, this may affect the test.

## Test Config For Prod-like Workload

To examine a prod-like config, please refer to these files.

```
cachelib/cachebench/test_configs/hit_ratio_test_configs/memcache_twmemcache.reg.altoona_model_hr.json
cachelib/cachebench/test_configs/hit_ratio_test_configs/memcache_twmemcache.reg.altoona_pop.json
cachelib/cachebench/test_configs/hit_ratio_test_configs/memcache_twmemcache.reg.altoona_sizes.json
```

The file that ends with `model_hr.json` is the source file that describes the basic cache setup (similar to how we do it in the simple test config example above). The `pop.json` file describes the popularity distribution across the key space and the `sizes.json` file describes the value distribution across the key space. They are key to getting close to simulate a production workload.
These files are generated by our workload analyzer (https://fburl.com/diffusion/zipyekcj). We currently have the ability to take the most recent Tao and Memcache traces and generate workload configs to simulate their caching behavior. For example to generate a workload config that simulates the most recent two days of traffic, simply run:

```
cd cachelib/workload_characterization/
./presto.sh && ./parse_traces.sh
```

## Reply Captured Production Traces

We can also use captured cachelib traces to replay the same lookup traffic from prod. This is as close as we can get to production and is very useful when we want to simulate a cache setup that is similar to production. To do that, we need to size our cache to the same scale as our trace. If the trace is sampled at 0.01% of the traffic across 1000 hosts, then we need a cache size that’s roughly 10% of production. Running the script above would also generate a config that uses the captured traces. For example, look at `cachelib/cachebench/test_configs/hit_ratio_test_configs/memcache_twmemcache.reg.altoona_trace_hr.json`.

```
{
  "cache_config": {
    "cacheSizeMB": 8192,
    "poolRebalanceIntervalSec": 0
  },
  "test_config":
    {
      "enableLookaside": true,
      "generator": "replay",
      "numOps": 240000000,
      "numThreads": 1,
      "prepopulateCache": true,
      "traceFileName": "memcache_twmemcache.reg.altoona.csv"
    }

}
```

Due to the size of trace file, we do not store the raw traces in cachebench repo. So to run the config above, the user must first fetch the raw trace locally and put it in the same directory as the config file.

# Write A New Workload Generator

Workload generator needs to implement an interface that CacheBench’s Cache implementation expects. The most important two APIs are as follows.

```
# Get a request for our next operation. The request contains key and size.
const Request& getReq(uint8_t poolId, std::mt19937& gen);

# Get an operation to go with the generated request (e.g. get, set, del)
OpType getOp(uint8_t pid, std::mt19937& gen);
```

For an example, please look at OnlineGenerator (https://fburl.com/diffusion/tpvf5ovt) which implements a generator that use distribution descriptions for popularity and value sizes to generates requests.
